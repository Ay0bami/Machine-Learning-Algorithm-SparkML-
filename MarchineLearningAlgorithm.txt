/*

-- This code analysis the prediction of how many resolved cases we have in COVID-19 cases
-- in Toronto using Random Forest in Machine Learning Algorithm 
-- By: Islamiyat Osuolale 

*/

-- Run spark-shell

spark-shell --master yarn

/*
-- Bunch of import statements
*/

import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}
import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}
import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.types.{IntegerType}
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.DataFrame
import org.apache.spark.SparkContext


/*

--Load dataset from HDFS and clean Null values

*/


val CovidCasesData1 = spark.read
.format("csv")
.option("header", "true")
.load("hdfs://10.128.0.3:8020/BigData/COVID19_cases.csv")
val CovidCasesData = CovidCasesData1.na.drop()

/* 

-- Showing the schema of the data set 

*/

CovidCasesData.show(10)
CovidCasesData.schema(10)



/*

-- Selecting the columns we want to use

*/


val CovidCasesDataset = CovidCasesData.select(col("Outcome"), col("Age Group"), col("Ever Hospitalized"), col("Ever Intubated"), col("Currently in ICU"))


val inputColumns = Array("Age Group","Ever Hospitalized","Ever Intubated","Currently in ICU")
val outputColumns = Array("AgeGroup_Index","EverHospitalized_Index","EverIntubated_Index","CurrentlyInICU_Index")

val indexer = new StringIndexer()
indexer.setInputCols(inputColumns)
indexer.setOutputCols(outputColumns)




val rankcovidCasesData = CovidCasesDataset.select(col("Outcome").cast(IntegerType),
col("Age Group").cast(IntegerType),
col("Ever Hospitalized").cast(IntegerType),
col("Ever Intubated").cast(IntegerType),
col("Currently in ICU").cast(IntegerType))

rankcovidCasesData.show(12)


/*

-- Split our dataset into training and test data typical 80 20
-- we give a seed so we have the same random data in each set

*/

val Array(trainingData, testData) = rankcovidCasesData.randomSplit(Array(0.8, 0.2), 754) 

 

val stringIndexer =new StringIndexer()
.setInputCol("Outcome")
.setOutputCol("Outcome_Index")



/*

--Assemble all features
-- Next we assemble our features using vector assembler, this time we have more than one feature

*/


val assembler = new VectorAssembler()
 .setInputCols(Array("Age Group","Ever Hospitalized","Ever Intubated","Currently in ICU"))
 .setOutputCol("assembled-features")
 
 

/*

--Random Forest  
-- We now create a new random forest object 
-- give features (as a vector)
-- give the label as Outcome_Index (can't use the string column)
-- Set our seed as 1234

*/



val rf = new RandomForestClassifier()
 .setFeaturesCol("assembled-features")
 .setLabelCol("Outcome_Index")
 .setSeed(1234)
  
  
  
/*

--Set up pipeline
-- Use pipepline to set our stages
-- So our stages are the string indexer, the vector assembler and the random forest classifier object

*/




val pipeline = new Pipeline()
  .setStages(Array(stringIndexer, assembler, rf))




/*

--To evaluate the model
-- Next we provide the evaluator
-- For regressoin we used RegressionEvaluator
-- the metric accuracy was simply a percentage
-- Here we are using MulticlassClassificationEvaluator
-- we compared candidate_indexed ot the prediction column
-- IF the match, prediction is good else it is unsuccession
-- metric "accuracy" is basically percentage of accurate results

*/





val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("Outcome_Index")
  .setPredictionCol("prediction")
  .setMetricName("accuracy")
  
  
  
  
/*

-- Here we are trying hyper parameters
-- First is maxDepth
-- maxDepth puts a limit on how deep we can construct the tree
-- Remember, we said that with decision trees main problem was overfitting the data
-- random forest does a better job but still, there is an issue if we allow the tree to be too deep
-- so good idea to set the max depth
-- Second is impurity which we give as entropy (there are other statistical methods to select features)

*/





val paramGrid = new ParamGridBuilder()  
  .addGrid(rf.maxDepth, Array(3, 4))
  .addGrid(rf.impurity, Array("entropy","gini")).build()




/*

--Cross validate model

-- Next we tie everything together with cross-validator
-- We set the pipeline for estimator
-- Multiclassevaluator for evaluator
-- Set the hyper parameters and the number of folds
-- Cross validator will divide the training data set into 3
-- Next each fold is coupled with the paramters for each type
-- Fold 1 is tried with max depth 3 and entropy and then fold 1 is again tried but this time with max depth 4 and entropy
-- the best model is picked

*/




val cross_validator = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(3)





/*

-- Train the model on training data
-- Gives us the best model.

*/




val cvModel = cross_validator.fit(trainingData)





/*

--Predict with test data
-- Transform testData with predictions

*/




val predictions = cvModel.transform(testData)





/*

--Evaluate the model
-- check with actual values and print accuracy

*/



val accuracy = evaluator.evaluate(predictions)

println("accuracy on test data = " + accuracy)




/*

-- Lastly, we will printout and save our prediction file

*/



predictions
.select(col("Outcome_Index"),
col("Age Group"),
col("Ever Hospitalized"),
col("Ever Intubated"),
col("Currently in ICU"),
col("prediction"))
.write
.format("csv")
.option("header", "true")
.save("hdfs://10.128.0.3:8020/BigData/prediction.csv")











